{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelli : BART, T5, Marian, mBART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!export HF_TOKEN='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'\n",
    "!git clone \"https://github.com/IBM/KPA_2021_shared_task\"\n",
    "\n",
    "!pip install datasets -q\n",
    "!pip install transformers -q\n",
    "!pip install sentencepiece -q\n",
    "!pip install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:{'tr_shape': (24454, 7), 'vl_shape': (4211, 7), 'ts_shape': (3923, 7)}\n"
     ]
    }
   ],
   "source": [
    "from libs.generate.kpa_functions import load_kpm_data\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n",
    "\n",
    "dataset_directory = \"KPA_2021_shared_task/kpm_data\"  # directory for dataset used for training and validation set\n",
    "testset_directory = \"KPA_2021_shared_task/test_data\" # directory for dataset used for testing set \n",
    "\n",
    "tr_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"train\")\n",
    "vl_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"dev\")\n",
    "ts_data, _, _, _ = load_kpm_data(gold_data_dir = testset_directory, subset=\"test\")\n",
    "logging.debug({\"tr_shape\":tr_data.shape,\"vl_shape\":vl_data.shape,\"ts_shape\":ts_data.shape})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:{'train': Dataset({\n",
      "    features: ['arg_id', 'key_point_id', 'argument', 'keypoint', 'topic', 'stance', 'arg_topic'],\n",
      "    num_rows: 24454\n",
      "}), 'validation': Dataset({\n",
      "    features: ['arg_id', 'key_point_id', 'argument', 'keypoint', 'topic', 'stance', 'arg_topic'],\n",
      "    num_rows: 4211\n",
      "}), 'test': Dataset({\n",
      "    features: ['arg_id', 'key_point_id', 'argument', 'keypoint', 'topic', 'stance', 'arg_topic'],\n",
      "    num_rows: 3923\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tr_dataset = Dataset(tr_data.to_arrow())\n",
    "vl_dataset = Dataset(vl_data.to_arrow())\n",
    "ts_dataset = Dataset(ts_data.to_arrow())\n",
    "\n",
    "data = {'train': tr_dataset, 'validation': vl_dataset, 'test': ts_dataset}\n",
    "logging.debug(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def rouge_n_score(reference, hypothesis, n=1):\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "    \n",
    "    reference_ngrams = [tuple(reference_tokens[i:i+n]) for i in range(len(reference_tokens)-n+1)]\n",
    "    hypothesis_ngrams = [tuple(hypothesis_tokens[i:i+n]) for i in range(len(hypothesis_tokens)-n+1)]\n",
    "    \n",
    "    reference_ngram_counts = Counter(reference_ngrams)\n",
    "    hypothesis_ngram_counts = Counter(hypothesis_ngrams)\n",
    "    \n",
    "    intersection = sum((reference_ngram_counts & hypothesis_ngram_counts).values())\n",
    "    union = sum(reference_ngram_counts.values()) + sum(hypothesis_ngram_counts.values())\n",
    "    \n",
    "    if union == 0:\n",
    "        rouge_score = 0.0\n",
    "    else:\n",
    "        rouge_score = intersection / union\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "def rouge_l_score(reference, hypothesis):\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "    \n",
    "    reference_length = len(reference_tokens)\n",
    "    hypothesis_length = len(hypothesis_tokens)\n",
    "    \n",
    "    lcs_matrix = np.zeros((reference_length+1, hypothesis_length+1))\n",
    "    \n",
    "    for i in range(1, reference_length+1):\n",
    "        for j in range(1, hypothesis_length+1):\n",
    "            if reference_tokens[i-1] == hypothesis_tokens[j-1]:\n",
    "                lcs_matrix[i][j] = lcs_matrix[i-1][j-1] + 1\n",
    "            else:\n",
    "                lcs_matrix[i][j] = max(lcs_matrix[i-1][j], lcs_matrix[i][j-1])\n",
    "    \n",
    "    rouge_score = lcs_matrix[reference_length][hypothesis_length] / reference_length\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "def rouge_score(reference, hypothesis, n=1):\n",
    "    rouge_n = rouge_n_score(reference, hypothesis, n)\n",
    "    rouge_l = rouge_l_score(reference, hypothesis)\n",
    "    \n",
    "    rouge_score = (rouge_n + rouge_l) / 2\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "# # Example usage:\n",
    "# reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "# hypothesis = \"A quick brown fox jumps over a lazy dog\"\n",
    "\n",
    "# rouge_1 = rouge_score(reference, hypothesis, n=1)\n",
    "# rouge_2 = rouge_score(reference, hypothesis, n=2)\n",
    "# rouge_l = rouge_l_score(reference, hypothesis)\n",
    "\n",
    "# print(\"ROUGE-1 score:\", rouge_1)\n",
    "# print(\"ROUGE-2 score:\", rouge_2)\n",
    "# print(\"ROUGE-L score:\", rouge_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:https://huggingface.co:443 \"HEAD /google/mt5-small/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:https://huggingface.co:443 \"HEAD /google/mt5-small/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/riccardoamadio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small',use_fast = False)       # import pre-trained MT5 tokenizer \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small')    # import pre-trained MT5 model\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_function(data_set,max_input_length=300,max_target_length=60,padding=\"max_length\"):\n",
    "\n",
    "    inputs = data_set['argument']   # get input column\n",
    "    targets = data_set['keypoint']  # get target column\n",
    "    \n",
    "    # add useful prefix to input, to tell the model which task has to perform\n",
    "    #prefix = \"summarize: \"\n",
    "    #inputs = [prefix + inp for inp in inputs]\n",
    "\n",
    "    # execute input tokenization\n",
    "    model_inputs = tokenizer(inputs, \n",
    "                             max_length = max_input_length,\n",
    "                             padding = padding,\n",
    "                             truncation = True)\n",
    "\n",
    "    # execute target tokenizatiion\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, \n",
    "                           max_length = max_target_length,\n",
    "                           padding = padding,\n",
    "                           truncation =True)\n",
    "    labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \n",
    "    # get predictions and labels and split them in different sentence\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # post-processing: ROUGE expects a newline after each sentence\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = rouge_l_score(reference=decoded_labels, hypothesis=decoded_preds)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Extract the median scores\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e8b58de5b5432492fdfecb0de667f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58e6edf6afd4d2e88ba9725be370819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a4a1e2fafb46e39bc903680b0e5adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: stance, key_point_id, keypoint, arg_topic, arg_id, topic, argument. If stance, key_point_id, keypoint, arg_topic, arg_id, topic, argument are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24454\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4587\n",
      "  Number of trainable parameters = 300176768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b170a57ea994391a2b0e854201d2c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# define checkpoint \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# runs fine-tuning and save fine-tuned model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     40\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# use model to predict new summary on test set \u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1842\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1840\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1842\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    356\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    362\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "# apply preprocessing procedure on TR, VL e TS set\n",
    "train_dataset = tr_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = vl_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = ts_dataset.map(preprocess_function,batched=True)\n",
    "\n",
    "# define datacollators objects to use for creating batches\n",
    "data_collator = default_data_collator\n",
    "max_target_length= 60\n",
    "\n",
    "# define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir    = 'output/',\n",
    "    learning_rate = 1e-5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs    = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    predict_with_generate = True\n",
    ")\n",
    "\n",
    "# initialize Trainer object\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args  = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset  = eval_dataset,\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "# model.cuda()      # pass model to GPU\n",
    "checkpoint = ''   # define checkpoint \n",
    "\n",
    "# runs fine-tuning and save fine-tuned model\n",
    "train_result = trainer.train(resume_from_checkpoint = None) \n",
    "trainer.save_model()\n",
    "\n",
    "# use model to predict new summary on test set \n",
    "test_results = trainer.predict(\n",
    "      test_dataset = test_dataset,\n",
    "      metric_key_prefix = \"test\",\n",
    "      max_length = max_target_length,\n",
    "      num_beams = 6)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
