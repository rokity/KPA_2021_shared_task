{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelli : BART, T5, Marian, mBART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!export HF_TOKEN='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'\n",
    "!git clone \"https://github.com/IBM/KPA_2021_shared_task\"\n",
    "\n",
    "!pip install datasets -q\n",
    "!pip install transformers -q\n",
    "!pip install sentencepiece -q\n",
    "!pip install rouge_score -q\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip libs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.generate.kpa_functions import load_kpm_data\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n",
    "\n",
    "dataset_directory = \"KPA_2021_shared_task/kpm_data\"  # directory for dataset used for training and validation set\n",
    "testset_directory = \"KPA_2021_shared_task/test_data\" # directory for dataset used for testing set\n",
    "\n",
    "tr_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"train\")\n",
    "vl_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"dev\")\n",
    "ts_data, _, _, _ = load_kpm_data(gold_data_dir = testset_directory, subset=\"test\")\n",
    "logging.debug({\"tr_shape\":tr_data.shape,\"vl_shape\":vl_data.shape,\"ts_shape\":ts_data.shape})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tr_dataset = Dataset(tr_data.to_arrow())\n",
    "vl_dataset = Dataset(vl_data.to_arrow())\n",
    "ts_dataset = Dataset(ts_data.to_arrow())\n",
    "\n",
    "data = {'train': tr_dataset, 'validation': vl_dataset, 'test': ts_dataset}\n",
    "logging.debug(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_rouge_n(reference, hypothesis, n):\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "    hypothesis_ngrams = list(ngrams(hypothesis_tokens, n))\n",
    "\n",
    "    reference_ngram_counter = Counter(reference_ngrams)\n",
    "    hypothesis_ngram_counter = Counter(hypothesis_ngrams)\n",
    "\n",
    "    overlap_count = sum((reference_ngram_counter & hypothesis_ngram_counter).values())\n",
    "    reference_count = sum(reference_ngram_counter.values())\n",
    "    \n",
    "    if reference_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    rouge_n_recall = overlap_count / reference_count\n",
    "\n",
    "    return rouge_n_recall\n",
    "\n",
    "def compute_rouge_l(reference_tokens, hypothesis_tokens):\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "    reference_length = len(reference_tokens)\n",
    "    hypothesis_length = len(hypothesis_tokens)\n",
    "\n",
    "    reference_set = set(reference_tokens)\n",
    "    hypothesis_set = set(hypothesis_tokens)\n",
    "\n",
    "    overlap_count = len(reference_set.intersection(hypothesis_set))\n",
    "    \n",
    "    if reference_length == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    rouge_l_recall = overlap_count / reference_length\n",
    "\n",
    "    return rouge_l_recall\n",
    "\n",
    "def compute_rouge(reference, hypothesis, n=1):\n",
    "    if n == 1:\n",
    "        return compute_rouge_l(reference, hypothesis)\n",
    "    else:\n",
    "        return compute_rouge_n(reference, hypothesis, n)\n",
    "\n",
    "def compute_rouge_scores(reference, hypothesis, max_n=4):\n",
    "    rouge_scores = {}\n",
    "    for n in range(1, max_n + 1):\n",
    "        rouge_scores[f'ROUGE-{n}'] = compute_rouge(reference, hypothesis, n)\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/riccardoamadio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small',use_fast = False)       # import pre-trained MT5 tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small')    # import pre-trained MT5 model\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_function(data_set,max_input_length=300,max_target_length=60,padding=\"max_length\"):\n",
    "\n",
    "    inputs = data_set['argument']   # get input column\n",
    "    targets = data_set['keypoint']  # get target column\n",
    "\n",
    "    # add useful prefix to input, to tell the model which task has to perform\n",
    "    #prefix = \"summarize: \"\n",
    "    #inputs = [prefix + inp for inp in inputs]\n",
    "\n",
    "    # execute input tokenization\n",
    "    model_inputs = tokenizer(inputs,\n",
    "                             max_length = max_input_length,\n",
    "                             padding = padding,\n",
    "                             truncation = True)\n",
    "\n",
    "    # execute target tokenizatiion\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets,\n",
    "                           max_length = max_target_length,\n",
    "                           padding = padding,\n",
    "                           truncation =True)\n",
    "    labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "    # get predictions and labels and split them in different sentence\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # post-processing: ROUGE expects a newline after each sentence\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    rouge_scores = []\n",
    "\n",
    "    # Compute ROUGE scores for each pair of generated and reference summaries\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        rouge_scores.append(compute_rouge_scores(label, pred))\n",
    "\n",
    "    # Combine ROUGE scores from all pairs\n",
    "    avg_rouge_scores = {}\n",
    "    for rouge_score in rouge_scores:\n",
    "        for metric, score in rouge_score.items():\n",
    "            avg_rouge_scores.setdefault(metric, []).append(score)\n",
    "\n",
    "    # Compute the average ROUGE scores\n",
    "    for metric, scores in avg_rouge_scores.items():\n",
    "        avg_rouge_scores[metric] = np.mean(scores)\n",
    "\n",
    "    # Extract the median lengths of generated summaries\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    avg_rouge_scores[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Round the scores for better readability\n",
    "    avg_rouge_scores = {k: round(v, 4) for k, v in avg_rouge_scores.items()}\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852b22d293c2435f80892a24aa5307c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf165aec93847eda09ced803d8bac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c511cfe1fe624bc89ec1c3d677e642be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: topic, arg_id, stance, arg_topic, argument, key_point_id, keypoint. If topic, arg_id, stance, arg_topic, argument, key_point_id, keypoint are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24454\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4587\n",
      "  Number of trainable parameters = 300176768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf3d25d61664d0e814761634b05179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# define checkpoint \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# runs fine-tuning and save fine-tuned model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     44\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# use model to predict new summary on test set \u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1778\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:2541\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2541\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "import torch\n",
    "# apply preprocessing procedure on TR, VL e TS set\n",
    "train_dataset = tr_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = vl_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = ts_dataset.map(preprocess_function,batched=True)\n",
    "\n",
    "#mps_device = torch.device(\"mps\")\n",
    "#model.to(mps_device)\n",
    "# define datacollators objects to use for creating batches\n",
    "data_collator = default_data_collator\n",
    "max_target_length= 60\n",
    "\n",
    "# define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir    = 'output/',\n",
    "    learning_rate = 1e-5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs    = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    predict_with_generate = True\n",
    ")\n",
    "\n",
    "# initialize Trainer object\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args  = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset  = eval_dataset,\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "model.cuda()     # pass model to GPU\n",
    "checkpoint = ''   # define checkpoint\n",
    "\n",
    "# runs fine-tuning and save fine-tuned model\n",
    "train_result = trainer.train(resume_from_checkpoint = None)\n",
    "trainer.save_model()\n",
    "\n",
    "# use model to predict new summary on test set\n",
    "test_results = trainer.predict(\n",
    "      test_dataset = test_dataset,\n",
    "      metric_key_prefix = \"test\",\n",
    "      max_length = max_target_length,\n",
    "      num_beams = 6)\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
