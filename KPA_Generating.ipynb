{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelli : BART, T5, Marian, mBART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!export HF_TOKEN='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'\n",
    "!git clone \"https://github.com/IBM/KPA_2021_shared_task\"\n",
    "\n",
    "!pip install datasets -q\n",
    "!pip install transformers -q\n",
    "!pip install sentencepiece -q\n",
    "!pip install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.generate.kpa_functions import load_kpm_data\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.DEBUG)\n",
    "\n",
    "dataset_directory = \"KPA_2021_shared_task/kpm_data\"  # directory for dataset used for training and validation set\n",
    "testset_directory = \"KPA_2021_shared_task/test_data\" # directory for dataset used for testing set \n",
    "\n",
    "tr_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"train\")\n",
    "vl_data, _, _, _ = load_kpm_data(gold_data_dir = dataset_directory, subset = \"dev\")\n",
    "ts_data, _, _, _ = load_kpm_data(gold_data_dir = testset_directory, subset=\"test\")\n",
    "logging.debug({\"tr_shape\":tr_data.shape,\"vl_shape\":vl_data.shape,\"ts_shape\":ts_data.shape})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tr_dataset = Dataset(tr_data.to_arrow())\n",
    "vl_dataset = Dataset(vl_data.to_arrow())\n",
    "ts_dataset = Dataset(ts_data.to_arrow())\n",
    "\n",
    "data = {'train': tr_dataset, 'validation': vl_dataset, 'test': ts_dataset}\n",
    "logging.debug(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def rouge_n_score(reference, hypothesis, n=1):\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "    \n",
    "    reference_ngrams = [tuple(reference_tokens[i:i+n]) for i in range(len(reference_tokens)-n+1)]\n",
    "    hypothesis_ngrams = [tuple(hypothesis_tokens[i:i+n]) for i in range(len(hypothesis_tokens)-n+1)]\n",
    "    \n",
    "    reference_ngram_counts = Counter(reference_ngrams)\n",
    "    hypothesis_ngram_counts = Counter(hypothesis_ngrams)\n",
    "    \n",
    "    intersection = sum((reference_ngram_counts & hypothesis_ngram_counts).values())\n",
    "    union = sum(reference_ngram_counts.values()) + sum(hypothesis_ngram_counts.values())\n",
    "    \n",
    "    if union == 0:\n",
    "        rouge_score = 0.0\n",
    "    else:\n",
    "        rouge_score = intersection / union\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "def rouge_l_score(reference, hypothesis):\n",
    "    reference_tokens = word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = word_tokenize(hypothesis.lower())\n",
    "    \n",
    "    reference_length = len(reference_tokens)\n",
    "    hypothesis_length = len(hypothesis_tokens)\n",
    "    \n",
    "    lcs_matrix = np.zeros((reference_length+1, hypothesis_length+1))\n",
    "    \n",
    "    for i in range(1, reference_length+1):\n",
    "        for j in range(1, hypothesis_length+1):\n",
    "            if reference_tokens[i-1] == hypothesis_tokens[j-1]:\n",
    "                lcs_matrix[i][j] = lcs_matrix[i-1][j-1] + 1\n",
    "            else:\n",
    "                lcs_matrix[i][j] = max(lcs_matrix[i-1][j], lcs_matrix[i][j-1])\n",
    "    \n",
    "    rouge_score = lcs_matrix[reference_length][hypothesis_length] / reference_length\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "def rouge_score(reference, hypothesis, n=1):\n",
    "    rouge_n = rouge_n_score(reference, hypothesis, n)\n",
    "    rouge_l = rouge_l_score(reference, hypothesis)\n",
    "    \n",
    "    rouge_score = (rouge_n + rouge_l) / 2\n",
    "    \n",
    "    return rouge_score\n",
    "\n",
    "# # Example usage:\n",
    "# reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "# hypothesis = \"A quick brown fox jumps over a lazy dog\"\n",
    "\n",
    "# rouge_1 = rouge_score(reference, hypothesis, n=1)\n",
    "# rouge_2 = rouge_score(reference, hypothesis, n=2)\n",
    "# rouge_l = rouge_l_score(reference, hypothesis)\n",
    "\n",
    "# print(\"ROUGE-1 score:\", rouge_1)\n",
    "# print(\"ROUGE-2 score:\", rouge_2)\n",
    "# print(\"ROUGE-L score:\", rouge_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/riccardoamadio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small',use_fast = False)       # import pre-trained MT5 tokenizer \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small')    # import pre-trained MT5 model\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_function(data_set,max_input_length=300,max_target_length=60,padding=\"max_length\"):\n",
    "\n",
    "    inputs = data_set['argument']   # get input column\n",
    "    targets = data_set['keypoint']  # get target column\n",
    "    \n",
    "    # add useful prefix to input, to tell the model which task has to perform\n",
    "    #prefix = \"summarize: \"\n",
    "    #inputs = [prefix + inp for inp in inputs]\n",
    "\n",
    "    # execute input tokenization\n",
    "    model_inputs = tokenizer(inputs, \n",
    "                             max_length = max_input_length,\n",
    "                             padding = padding,\n",
    "                             truncation = True)\n",
    "\n",
    "    # execute target tokenizatiion\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, \n",
    "                           max_length = max_target_length,\n",
    "                           padding = padding,\n",
    "                           truncation =True)\n",
    "    labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \n",
    "    # get predictions and labels and split them in different sentence\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # post-processing: ROUGE expects a newline after each sentence\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = rouge_l_score(reference=decoded_labels, hypothesis=decoded_preds)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Extract the median scores\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852b22d293c2435f80892a24aa5307c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf165aec93847eda09ced803d8bac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c511cfe1fe624bc89ec1c3d677e642be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: topic, arg_id, stance, arg_topic, argument, key_point_id, keypoint. If topic, arg_id, stance, arg_topic, argument, key_point_id, keypoint are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24454\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4587\n",
      "  Number of trainable parameters = 300176768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf3d25d61664d0e814761634b05179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "import torch\n",
    "# apply preprocessing procedure on TR, VL e TS set\n",
    "train_dataset = tr_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = vl_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = ts_dataset.map(preprocess_function,batched=True)\n",
    "\n",
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)\n",
    "# define datacollators objects to use for creating batches\n",
    "data_collator = default_data_collator\n",
    "max_target_length= 60\n",
    "\n",
    "# define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir    = 'output/',\n",
    "    learning_rate = 1e-5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs    = 3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    predict_with_generate = True\n",
    ")\n",
    "\n",
    "# initialize Trainer object\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args  = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset  = eval_dataset,\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# model.cuda()     # pass model to GPU\n",
    "checkpoint = ''   # define checkpoint \n",
    "\n",
    "# runs fine-tuning and save fine-tuned model\n",
    "train_result = trainer.train(resume_from_checkpoint = None) \n",
    "trainer.save_model()\n",
    "\n",
    "# use model to predict new summary on test set \n",
    "test_results = trainer.predict(\n",
    "      test_dataset = test_dataset,\n",
    "      metric_key_prefix = \"test\",\n",
    "      max_length = max_target_length,\n",
    "      num_beams = 6)\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
