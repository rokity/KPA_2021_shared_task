{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 2: Key Point Matching [Bert-Base]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings & Initialization\n",
    "\n",
    "⋅ Install tools and dependencies. \n",
    "\n",
    "⋅ Set Environemnt Variables. \n",
    "\n",
    "⋅ Define Constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/huggingface_hub-0.12.0rc0-py3.8.egg (from transformers) (0.12.0rc0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: optuna in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: colorlog in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: PyYAML in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (2.0.7)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (1.24.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Requirement already satisfied: Mako in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding,TrainingArguments,default_data_collator,EvalPrediction,AutoModelForSequenceClassification,AutoTokenizer,Trainer\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "ID2LABEL = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "LABEL2ID = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "GOLD_DATA_DIR = './../kpm_data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & PostProcessing Methods\n",
    " \n",
    "\n",
    "⋅ Load Raw data by using the protocol defined on paper ArgMining KPA 2021 Shared task.\n",
    "\n",
    "⋅ Tokenize function to apply before training by using pre-trained tokenizer *distilbert-base-uncased*\n",
    "\n",
    "⋅ Merge labels,topic and arguments in the same dataframe\n",
    "\n",
    "⋅ Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kpm_data(subset, submitted_kp_file=None,nrows=None):\n",
    "    print(\"\\nֿ** loading task data:\")\n",
    "    arguments_file = os.path.join(GOLD_DATA_DIR, f\"arguments_{subset}.csv\")\n",
    "    if not submitted_kp_file:\n",
    "        key_points_file = os.path.join(GOLD_DATA_DIR, f\"key_points_{subset}.csv\")\n",
    "    else:\n",
    "        key_points_file=submitted_kp_file\n",
    "    labels_file = os.path.join(GOLD_DATA_DIR, f\"labels_{subset}.csv\")\n",
    "    arguments_df = pd.read_csv(arguments_file,nrows=nrows)\n",
    "    key_points_df = pd.read_csv(key_points_file,nrows=nrows)\n",
    "    labels_file_df = pd.read_csv(labels_file,nrows=nrows)\n",
    "\n",
    "    return arguments_df, key_points_df, labels_file_df\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    token = tokenizer(examples[\"key_point\"],examples[\"argument\"],\n",
    "     truncation=True)\n",
    "    token[\"labels\"] = examples[\"label\"]\n",
    "    return token\n",
    "\n",
    "def merge_df(arg_df, kp_df, labels_df):\n",
    "    labels_df=labels_df.merge(kp_df,on=\"key_point_id\",how=\"left\")\n",
    "    labels_df=labels_df.merge(arg_df,on='arg_id',how=\"left\")\n",
    "    labels_df.drop([\"stance_x\", \"topic_x\"], axis=1, inplace=True)\n",
    "    labels_df=labels_df.rename(columns={\"stance_y\":\"stance\", \"topic_y\":\"topic\"})\n",
    "    return labels_df\n",
    "\n",
    "def remove_nan(labels_df):\n",
    "    labels_df = labels_df[labels_df['argument'].notna()]\n",
    "    labels_df = labels_df[labels_df['topic'].notna()]\n",
    "    labels_df = labels_df[labels_df['stance'].notna()]\n",
    "    return labels_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "⋅ Set configuration of hyperparameter search with [optuna](https://www.example.com)\n",
    "\n",
    "⋅ Initialize model for the grid search\n",
    "\n",
    "⋅ Retrieve evaluations metrics to select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-8, 1e-6, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2,4,8]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",num_labels=2, id2label=ID2LABEL, label2id=LABEL2ID)\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[:, 0]\n",
    "    predictions = np.where(predictions < 0.5, 0, 1)\n",
    "    _precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    _map = average_precision_score(labels, predictions)\n",
    "    _recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    _f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    _accuracy= accuracy.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "                \"precision\": _precision,\n",
    "                \"recall\": _recall,\n",
    "                \"f1\": _f1,\n",
    "                \"accuracy\": _accuracy,\n",
    "                \"map\": _map,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ֿ** loading task data:\n",
      "\n",
      "ֿ** loading task data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20635\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2580\n",
      "  Number of trainable parameters = 66955010\n",
      "  0%|          | 0/2580 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 19%|█▉        | 500/2580 [03:50<16:14,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4191, 'learning_rate': 1.612403100775194e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1000/2580 [07:42<12:39,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3302, 'learning_rate': 1.2248062015503876e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1290/2580 [09:57<09:11,  2.34it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.0888797023563456}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.29132791327913277}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.1362052581564777}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.21139386928860612}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 50%|█████     | 1290/2580 [10:21<09:11,  2.34it/s]Saving model checkpoint to my_awesome_model/checkpoint-1290\n",
      "Configuration saved in my_awesome_model/checkpoint-1290/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.375709593296051, 'eval_precision': {'precision': 0.0888797023563456}, 'eval_recall': {'recall': 0.29132791327913277}, 'eval_f1': {'f1': 0.1362052581564777}, 'eval_accuracy': {'accuracy': 0.21139386928860612}, 'eval_map': 0.17713663156910112, 'eval_runtime': 24.328, 'eval_samples_per_second': 142.141, 'eval_steps_per_second': 8.92, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/checkpoint-1290/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/checkpoint-1290/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/checkpoint-1290/special_tokens_map.json\n",
      " 58%|█████▊    | 1499/2580 [16:19<08:49,  2.04it/s]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m\n\u001b[1;32m     44\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     45\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     46\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[39m#     trainer = Trainer(\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m#     model=None,\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m#     args=training_args,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m#     compute_objective=compute_objective,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     71\u001b[0m     metrics \u001b[39m=\u001b[39m train_result\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m     72\u001b[0m     max_train_samples \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(result_train))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1532\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1778\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1779\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1780\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/trainer.py:2541\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2540\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2541\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2543\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",num_labels=2, id2label=ID2LABEL, label2id=LABEL2ID)\n",
    "    \n",
    "    # Merge Labels with arguments and key_points for training dataset\n",
    "    arg_df_dev, kp_df_dev, labels_df_dev = load_kpm_data( subset=\"train\")\n",
    "    labels_df_dev=merge_df(arg_df_dev, kp_df_dev, labels_df_dev)\n",
    "    # Merge Labels with arguments and key_points for test dataset\n",
    "    arg_df_test, kp_df_test, labels_df_test = load_kpm_data(subset=\"dev\")\n",
    "    labels_df_test=merge_df(arg_df_test, kp_df_test, labels_df_test)\n",
    "    # labels_df_test=labels_df_test.merge(kp_df_test,on=\"key_point_id\",how=\"left\")\n",
    "    # labels_df_test=labels_df_test.merge(arg_df_test,on='arg_id',how=\"left\")\n",
    "    # labels_df_test.drop([\"stance_x\", \"topic_x\"], axis=1, inplace=True)\n",
    "    # labels_df_test=labels_df_test.rename(columns={\"stance_y\":\"stance\", \"topic_y\":\"topic\"})\n",
    "    #encode each sentence and append to dictionary\n",
    "    # filter where labels are not nan\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['argument'].notna()]\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['topic'].notna()]\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['stance'].notna()]\n",
    "    labels_df_dev = remove_nan(labels_df_dev)\n",
    "    labels_df_test = remove_nan(labels_df_test)\n",
    "    # # labels_df_test = labels_df_test[labels_df_test['argument'].notna()]\n",
    "    # # labels_df_test = labels_df_test[labels_df_test['topic'].notna()]\n",
    "    # # labels_df_test = labels_df_test[labels_df_test['stance'].notna()]\n",
    "    result_train = labels_df_dev.apply(preprocess_function,axis=1)    \n",
    "    result_test = labels_df_test.apply(preprocess_function,axis=1)\n",
    "  \n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=result_train,\n",
    "    eval_dataset=result_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#     trainer = Trainer(\n",
    "#     model=None,\n",
    "#     args=training_args,\n",
    "#     train_dataset=result_train,\n",
    "#     eval_dataset=result_test,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,\n",
    "#     model_init=model_init,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "#     train_result = trainer.hyperparameter_search(\n",
    "#     direction=\"maximize\",\n",
    "#     backend=\"optuna\",\n",
    "#     hp_space=optuna_hp_space,\n",
    "#     n_trials=20,\n",
    "#     compute_objective=compute_objective,\n",
    "# )\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (len(result_train))\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(result_train))\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
