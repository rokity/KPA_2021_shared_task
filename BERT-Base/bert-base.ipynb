{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 2: Key Point Matching [Bert-Base]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings & Initialization\n",
    "\n",
    "⋅ Install tools and dependencies. \n",
    "\n",
    "⋅ Set Environemnt Variables. \n",
    "\n",
    "⋅ Define Constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/huggingface_hub-0.12.0rc0-py3.8.egg (from transformers) (0.12.0rc0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: optuna in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: PyYAML in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (1.10.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (1.24.1)\n",
      "Requirement already satisfied: colorlog in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from optuna) (2.0.7)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.11.3)\n",
      "Requirement already satisfied: Mako in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding,TrainingArguments,default_data_collator,EvalPrediction,AutoModelForSequenceClassification,AutoTokenizer,Trainer\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "ID2LABEL = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "LABEL2ID = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "GOLD_DATA_DIR = './../kpm_data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & PostProcessing Methods\n",
    " \n",
    "\n",
    "⋅ Load Raw data by using the protocol defined on paper ArgMining KPA 2021 Shared task.\n",
    "\n",
    "⋅ Tokenize function to apply before training by using pre-trained tokenizer *distilbert-base-uncased*\n",
    "\n",
    "⋅ Merge labels,topic and arguments in the same dataframe\n",
    "\n",
    "⋅ Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kpm_data(subset, submitted_kp_file=None,nrows=None):\n",
    "    print(\"\\nֿ** loading task data:\")\n",
    "    arguments_file = os.path.join(GOLD_DATA_DIR, f\"arguments_{subset}.csv\")\n",
    "    if not submitted_kp_file:\n",
    "        key_points_file = os.path.join(GOLD_DATA_DIR, f\"key_points_{subset}.csv\")\n",
    "    else:\n",
    "        key_points_file=submitted_kp_file\n",
    "    labels_file = os.path.join(GOLD_DATA_DIR, f\"labels_{subset}.csv\")\n",
    "    arguments_df = pd.read_csv(arguments_file,nrows=nrows)\n",
    "    key_points_df = pd.read_csv(key_points_file,nrows=nrows)\n",
    "    labels_file_df = pd.read_csv(labels_file,nrows=nrows)\n",
    "\n",
    "    return arguments_df, key_points_df, labels_file_df\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    token = tokenizer(examples[4]+examples[5],examples[3],\n",
    "     truncation=True)\n",
    "    token[\"labels\"] = examples[2]\n",
    "    return token\n",
    "\n",
    "def merge_df(arg_df, kp_df, labels_df):\n",
    "    labels_df=labels_df.merge(kp_df,on=\"key_point_id\",how=\"left\")\n",
    "    labels_df=labels_df.merge(arg_df,on='arg_id',how=\"left\")\n",
    "    labels_df.drop([\"stance_x\", \"topic_x\"], axis=1, inplace=True)\n",
    "    labels_df=labels_df.rename(columns={\"stance_y\":\"stance\", \"topic_y\":\"topic\"})\n",
    "    return labels_df\n",
    "\n",
    "def remove_nan(labels_df):\n",
    "    labels_df = labels_df[labels_df['argument'].notna()]\n",
    "    labels_df = labels_df[labels_df['topic'].notna()]\n",
    "    labels_df = labels_df[labels_df['stance'].notna()]\n",
    "    return labels_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "⋅ Set configuration of hyperparameter search with [optuna](https://www.example.com)\n",
    "\n",
    "⋅ Initialize model for the grid search\n",
    "\n",
    "⋅ Retrieve evaluations metrics to select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "        \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-8, 1e-6, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2,4,8]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "    }\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",num_labels=2, id2label=ID2LABEL, label2id=LABEL2ID)\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[:, 0]\n",
    "    predictions = np.where(predictions < 0.5, 0, 1)\n",
    "    _precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    _map = average_precision_score(labels, predictions)\n",
    "    _recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    _f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    _accuracy= accuracy.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "                \"precision\": _precision,\n",
    "                \"recall\": _recall,\n",
    "                \"f1\": _f1,\n",
    "                \"accuracy\": _accuracy,\n",
    "                \"map\": _map,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ֿ** loading task data:\n",
      "\n",
      "ֿ** loading task data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m[I 2023-03-22 18:06:15,897]\u001b[0m A new study created in memory with name: no-name-c73dfc75-9570-493e-b2d7-15ec96f93767\u001b[0m\n",
      "Trying to set batch_size in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trial: {'weight_decay': 0.015288100808666983, 'adam_epsilon': 3.047867930584892e-07, 'num_train_epochs': 8, 'batch_size': 16, 'learning_rate': 6.062816773570408e-05}\n",
      "loading configuration file config.json from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20635\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10320\n",
      "  Number of trainable parameters = 66955010\n",
      "  0%|          | 0/10320 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  5%|▍         | 500/10320 [04:01<1:22:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4535, 'learning_rate': 5.769075650819904e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1000/10320 [08:09<1:17:22,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3646, 'learning_rate': 5.4753345280694e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1290/10320 [10:34<1:07:49,  2.22it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08774439675727229}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.24932249322493225}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.12980599647266314}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.2865818392134182}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                      \n",
      " 12%|█▎        | 1290/10320 [11:04<1:07:49,  2.22it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-1290\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-1290/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5613069534301758, 'eval_precision': {'precision': 0.08774439675727229}, 'eval_recall': {'recall': 0.24932249322493225}, 'eval_f1': {'f1': 0.12980599647266314}, 'eval_accuracy': {'accuracy': 0.2865818392134182}, 'eval_map': 0.18208486460583256, 'eval_runtime': 29.4482, 'eval_samples_per_second': 117.427, 'eval_steps_per_second': 7.369, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-1290/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-1290/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-1290/special_tokens_map.json\n",
      " 15%|█▍        | 1500/10320 [12:50<1:15:52,  1.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3129, 'learning_rate': 5.181593405318895e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2000/10320 [17:01<1:10:11,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2587, 'learning_rate': 4.8878522825683905e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2500/10320 [21:14<1:03:35,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2636, 'learning_rate': 4.5941111598178865e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2580/10320 [21:55<1:01:23,  2.10it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.09053833605220228}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.3008130081300813}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.13918495297805641}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.20589936379410065}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                      \n",
      " 25%|██▌       | 2580/10320 [22:24<1:01:23,  2.10it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-2580\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-2580/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5370359420776367, 'eval_precision': {'precision': 0.09053833605220228}, 'eval_recall': {'recall': 0.3008130081300813}, 'eval_f1': {'f1': 0.13918495297805641}, 'eval_accuracy': {'accuracy': 0.20589936379410065}, 'eval_map': 0.17645431106973597, 'eval_runtime': 29.766, 'eval_samples_per_second': 116.173, 'eval_steps_per_second': 7.29, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-2580/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-2580/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-2580/special_tokens_map.json\n",
      " 29%|██▉       | 3000/10320 [26:00<1:01:41,  1.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1925, 'learning_rate': 4.3003700370673825e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3500/10320 [30:13<56:52,  2.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1743, 'learning_rate': 4.006628914316878e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3870/10320 [33:20<50:47,  2.12it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.09120788824979457}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.3008130081300813}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.13997477931904162}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.21110468478889532}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                    \n",
      " 38%|███▊      | 3870/10320 [33:49<50:47,  2.12it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-3870\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-3870/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5762094855308533, 'eval_precision': {'precision': 0.09120788824979457}, 'eval_recall': {'recall': 0.3008130081300813}, 'eval_f1': {'f1': 0.13997477931904162}, 'eval_accuracy': {'accuracy': 0.21110468478889532}, 'eval_map': 0.17665572108039382, 'eval_runtime': 29.2727, 'eval_samples_per_second': 118.131, 'eval_steps_per_second': 7.413, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-3870/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-3870/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-3870/special_tokens_map.json\n",
      " 39%|███▉      | 4000/10320 [34:56<52:41,  2.00it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1649, 'learning_rate': 3.712887791566374e-05, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4500/10320 [44:43<47:36,  2.04it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1273, 'learning_rate': 3.41914666881587e-05, 'epoch': 3.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 5000/10320 [48:54<45:08,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1255, 'learning_rate': 3.125405546065366e-05, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5160/10320 [50:14<41:42,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.0830656264341441}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.24525745257452575}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.12410010284538908}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.2611336032388664}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                    \n",
      " 50%|█████     | 5160/10320 [50:43<41:42,  2.06it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-5160\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-5160/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0041910409927368, 'eval_precision': {'precision': 0.0830656264341441}, 'eval_recall': {'recall': 0.24525745257452575}, 'eval_f1': {'f1': 0.12410010284538908}, 'eval_accuracy': {'accuracy': 0.2611336032388664}, 'eval_map': 0.18144823027466964, 'eval_runtime': 29.5688, 'eval_samples_per_second': 116.947, 'eval_steps_per_second': 7.339, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-5160/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-5160/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-5160/special_tokens_map.json\n",
      " 53%|█████▎    | 5500/10320 [53:34<41:23,  1.94it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0868, 'learning_rate': 2.831664423314861e-05, 'epoch': 4.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6000/10320 [57:44<35:53,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0682, 'learning_rate': 2.537923300564357e-05, 'epoch': 4.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 6450/10320 [1:01:27<29:44,  2.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08021631365479946}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.24119241192411925}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.12039228948258371}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.2478311162521689}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                      \n",
      " 62%|██████▎   | 6450/10320 [1:01:56<29:44,  2.17it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-6450\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-6450/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0173324346542358, 'eval_precision': {'precision': 0.08021631365479946}, 'eval_recall': {'recall': 0.24119241192411925}, 'eval_f1': {'f1': 0.12039228948258371}, 'eval_accuracy': {'accuracy': 0.2478311162521689}, 'eval_map': 0.1812908860041194, 'eval_runtime': 29.2824, 'eval_samples_per_second': 118.092, 'eval_steps_per_second': 7.411, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-6450/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-6450/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-6450/special_tokens_map.json\n",
      " 63%|██████▎   | 6500/10320 [1:02:22<30:33,  2.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.076, 'learning_rate': 2.2441821778138526e-05, 'epoch': 5.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 7000/10320 [1:06:32<27:56,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0417, 'learning_rate': 1.9504410550633483e-05, 'epoch': 5.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7500/10320 [1:10:41<22:36,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0471, 'learning_rate': 1.656699932312844e-05, 'epoch': 5.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7740/10320 [1:12:42<21:33,  1.99it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08931860036832412}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.26287262872628725}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.13333333333333333}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.2706766917293233}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                      \n",
      " 75%|███████▌  | 7740/10320 [1:13:12<21:33,  1.99it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-7740\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-7740/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1965614557266235, 'eval_precision': {'precision': 0.08931860036832412}, 'eval_recall': {'recall': 0.26287262872628725}, 'eval_f1': {'f1': 0.13333333333333333}, 'eval_accuracy': {'accuracy': 0.2706766917293233}, 'eval_map': 0.18079578311565772, 'eval_runtime': 29.961, 'eval_samples_per_second': 115.417, 'eval_steps_per_second': 7.243, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-7740/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-7740/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-7740/special_tokens_map.json\n",
      " 78%|███████▊  | 8000/10320 [1:15:23<19:18,  2.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0372, 'learning_rate': 1.3629588095623399e-05, 'epoch': 6.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8500/10320 [1:40:18<15:11,  2.00it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0296, 'learning_rate': 1.0692176868118355e-05, 'epoch': 6.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 9000/10320 [1:44:26<11:27,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0304, 'learning_rate': 7.754765640613313e-06, 'epoch': 6.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 9030/10320 [1:44:40<09:43,  2.21it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08388625592417062}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.23983739837398374}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.12429775280898878}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.27877385772122615}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                      \n",
      " 88%|████████▊ | 9030/10320 [1:45:11<09:43,  2.21it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-9030\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-9030/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6605541706085205, 'eval_precision': {'precision': 0.08388625592417062}, 'eval_recall': {'recall': 0.23983739837398374}, 'eval_f1': {'f1': 0.12429775280898878}, 'eval_accuracy': {'accuracy': 0.27877385772122615}, 'eval_map': 0.18235156571795477, 'eval_runtime': 30.4271, 'eval_samples_per_second': 113.649, 'eval_steps_per_second': 7.132, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-9030/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-9030/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-9030/special_tokens_map.json\n",
      " 92%|█████████▏| 9500/10320 [1:49:09<07:05,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0203, 'learning_rate': 4.81735441310827e-06, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 10000/10320 [1:53:23<02:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0178, 'learning_rate': 1.8799431856032273e-06, 'epoch': 7.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10320/10320 [1:56:05<00:00,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08341232227488152}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.23848238482384823}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.12359550561797752}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.2781954887218045}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                       \n",
      "100%|██████████| 10320/10320 [1:56:34<00:00,  2.12it/s]Saving model checkpoint to my_awesome_model/run-0/checkpoint-10320\n",
      "Configuration saved in my_awesome_model/run-0/checkpoint-10320/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6978628635406494, 'eval_precision': {'precision': 0.08341232227488152}, 'eval_recall': {'recall': 0.23848238482384823}, 'eval_f1': {'f1': 0.12359550561797752}, 'eval_accuracy': {'accuracy': 0.2781954887218045}, 'eval_map': 0.18241405837728747, 'eval_runtime': 29.3699, 'eval_samples_per_second': 117.74, 'eval_steps_per_second': 7.389, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-0/checkpoint-10320/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-0/checkpoint-10320/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-0/checkpoint-10320/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from my_awesome_model/run-0/checkpoint-2580 (score: 0.5370359420776367).\n",
      "100%|██████████| 10320/10320 [1:56:35<00:00,  1.48it/s]\n",
      "\u001b[32m[I 2023-03-22 20:02:52,692]\u001b[0m Trial 0 finished with value: 1.6978628635406494 and parameters: {'weight_decay': 0.015288100808666983, 'adam_epsilon': 3.047867930584892e-07, 'num_train_epochs': 8, 'batch_size': 16, 'learning_rate': 6.062816773570408e-05}. Best is trial 0 with value: 1.6978628635406494.\u001b[0m\n",
      "Trying to set batch_size in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trial: {'weight_decay': 0.0007245521341990524, 'adam_epsilon': 1.456775314454041e-08, 'num_train_epochs': 2, 'batch_size': 64, 'learning_rate': 0.0004028915465632688}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6995.8009, 'train_samples_per_second': 23.597, 'train_steps_per_second': 1.475, 'train_loss': 0.14054850058038104, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20635\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2580\n",
      "  Number of trainable parameters = 66955010\n",
      " 19%|█▉        | 500/2580 [04:11<17:33,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5152, 'learning_rate': 0.000324811789477364, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1000/2580 [08:24<12:49,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5126, 'learning_rate': 0.0002467320323914592, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1290/2580 [10:51<09:54,  2.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 1.0}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.3517635843660629}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 50%|█████     | 1290/2580 [11:22<09:54,  2.17it/s]Saving model checkpoint to my_awesome_model/run-1/checkpoint-1290\n",
      "Configuration saved in my_awesome_model/run-1/checkpoint-1290/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5191952586174011, 'eval_precision': {'precision': 0.21341816078658185}, 'eval_recall': {'recall': 1.0}, 'eval_f1': {'f1': 0.3517635843660629}, 'eval_accuracy': {'accuracy': 0.21341816078658185}, 'eval_map': 0.21341816078658185, 'eval_runtime': 31.2087, 'eval_samples_per_second': 110.803, 'eval_steps_per_second': 6.953, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-1/checkpoint-1290/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-1/checkpoint-1290/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-1/checkpoint-1290/special_tokens_map.json\n",
      " 58%|█████▊    | 1500/2580 [13:11<09:23,  1.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.51, 'learning_rate': 0.0001686522753055544, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2000/2580 [17:27<05:01,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5085, 'learning_rate': 9.057251821964958e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2500/2580 [21:47<00:39,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5142, 'learning_rate': 1.249276113374477e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2580/2580 [22:28<00:00,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 1.0}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.3517635843660629}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 2580/2580 [22:59<00:00,  2.07it/s]Saving model checkpoint to my_awesome_model/run-1/checkpoint-2580\n",
      "Configuration saved in my_awesome_model/run-1/checkpoint-2580/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5191100239753723, 'eval_precision': {'precision': 0.21341816078658185}, 'eval_recall': {'recall': 1.0}, 'eval_f1': {'f1': 0.3517635843660629}, 'eval_accuracy': {'accuracy': 0.21341816078658185}, 'eval_map': 0.21341816078658185, 'eval_runtime': 31.0259, 'eval_samples_per_second': 111.455, 'eval_steps_per_second': 6.994, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-1/checkpoint-2580/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-1/checkpoint-2580/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-1/checkpoint-2580/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from my_awesome_model/run-1/checkpoint-2580 (score: 0.5191100239753723).\n",
      "100%|██████████| 2580/2580 [23:01<00:00,  1.87it/s]\n",
      "\u001b[32m[I 2023-03-22 20:25:54,774]\u001b[0m Trial 1 finished with value: 0.5191100239753723 and parameters: {'weight_decay': 0.0007245521341990524, 'adam_epsilon': 1.456775314454041e-08, 'num_train_epochs': 2, 'batch_size': 64, 'learning_rate': 0.0004028915465632688}. Best is trial 0 with value: 1.6978628635406494.\u001b[0m\n",
      "Trying to set batch_size in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trial: {'weight_decay': 0.0014506341755737508, 'adam_epsilon': 1.9622876951047225e-07, 'num_train_epochs': 2, 'batch_size': 128, 'learning_rate': 1.6570962563348705e-05}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1381.0319, 'train_samples_per_second': 29.883, 'train_steps_per_second': 1.868, 'train_loss': 0.5126305336175963, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20635\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2580\n",
      "  Number of trainable parameters = 66955010\n",
      " 19%|█▉        | 500/2580 [04:04<17:38,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4161, 'learning_rate': 1.3359535709986552e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1000/2580 [08:08<12:30,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3302, 'learning_rate': 1.01481088566244e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1290/2580 [10:31<09:36,  2.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08855948308627898}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.3157181571815718}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.13831997625408132}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.1604973973395026}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 50%|█████     | 1290/2580 [11:01<09:36,  2.24it/s]Saving model checkpoint to my_awesome_model/run-2/checkpoint-1290\n",
      "Configuration saved in my_awesome_model/run-2/checkpoint-1290/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3605313003063202, 'eval_precision': {'precision': 0.08855948308627898}, 'eval_recall': {'recall': 0.3157181571815718}, 'eval_f1': {'f1': 0.13831997625408132}, 'eval_accuracy': {'accuracy': 0.1604973973395026}, 'eval_map': 0.1739980091549144, 'eval_runtime': 29.271, 'eval_samples_per_second': 118.137, 'eval_steps_per_second': 7.413, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-2/checkpoint-1290/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-2/checkpoint-1290/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-2/checkpoint-1290/special_tokens_map.json\n",
      " 58%|█████▊    | 1500/2580 [12:45<09:15,  1.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2891, 'learning_rate': 6.936682003262249e-06, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2000/2580 [16:52<05:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2432, 'learning_rate': 3.7252551499000966e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2500/2580 [20:59<00:39,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2487, 'learning_rate': 5.138282965379444e-07, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2580/2580 [21:39<00:00,  2.01it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.08895265423242468}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 0.33604336043360433}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.1406693136698809}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.12377096587622903}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      "100%|██████████| 2580/2580 [22:07<00:00,  2.01it/s]Saving model checkpoint to my_awesome_model/run-2/checkpoint-2580\n",
      "Configuration saved in my_awesome_model/run-2/checkpoint-2580/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37639763951301575, 'eval_precision': {'precision': 0.08895265423242468}, 'eval_recall': {'recall': 0.33604336043360433}, 'eval_f1': {'f1': 0.1406693136698809}, 'eval_accuracy': {'accuracy': 0.12377096587622903}, 'eval_map': 0.17159235370605208, 'eval_runtime': 28.0509, 'eval_samples_per_second': 123.276, 'eval_steps_per_second': 7.736, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-2/checkpoint-2580/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-2/checkpoint-2580/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-2/checkpoint-2580/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from my_awesome_model/run-2/checkpoint-1290 (score: 0.3605313003063202).\n",
      "100%|██████████| 2580/2580 [22:08<00:00,  1.94it/s]\n",
      "\u001b[32m[I 2023-03-22 20:48:04,447]\u001b[0m Trial 2 finished with value: 0.37639763951301575 and parameters: {'weight_decay': 0.0014506341755737508, 'adam_epsilon': 1.9622876951047225e-07, 'num_train_epochs': 2, 'batch_size': 128, 'learning_rate': 1.6570962563348705e-05}. Best is trial 0 with value: 1.6978628635406494.\u001b[0m\n",
      "Trying to set batch_size in the hyperparameter search but there is no corresponding field in `TrainingArguments`.\n",
      "Trial: {'weight_decay': 0.06622017300041948, 'adam_epsilon': 1.4420695167966294e-07, 'num_train_epochs': 2, 'batch_size': 64, 'learning_rate': 0.0002601888692799985}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1328.5297, 'train_samples_per_second': 31.064, 'train_steps_per_second': 1.942, 'train_loss': 0.30372829806897067, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/riccardoamadio/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/key_points/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20635\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2580\n",
      "  Number of trainable parameters = 66955010\n",
      " 19%|█▉        | 500/2580 [03:59<16:46,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5186, 'learning_rate': 0.00020976466980713058, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1000/2580 [07:57<11:52,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5129, 'learning_rate': 0.00015934047033426267, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1290/2580 [10:14<09:07,  2.36it/s]***** Running Evaluation *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 16\n",
      "Trainer is attempting to log a value of \"{'precision': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/precision\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'recall': 1.0}\" of type <class 'dict'> for key \"eval/recall\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'f1': 0.3517635843660629}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.21341816078658185}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 50%|█████     | 1290/2580 [10:42<09:07,  2.36it/s]Saving model checkpoint to my_awesome_model/run-3/checkpoint-1290\n",
      "Configuration saved in my_awesome_model/run-3/checkpoint-1290/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5184609889984131, 'eval_precision': {'precision': 0.21341816078658185}, 'eval_recall': {'recall': 1.0}, 'eval_f1': {'f1': 0.3517635843660629}, 'eval_accuracy': {'accuracy': 0.21341816078658185}, 'eval_map': 0.21341816078658185, 'eval_runtime': 27.9131, 'eval_samples_per_second': 123.885, 'eval_steps_per_second': 7.774, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_model/run-3/checkpoint-1290/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/run-3/checkpoint-1290/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/run-3/checkpoint-1290/special_tokens_map.json\n",
      " 58%|█████▊    | 1500/2580 [12:22<08:50,  2.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5107, 'learning_rate': 0.00010891627086139474, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2000/2580 [16:18<04:32,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5095, 'learning_rate': 5.8492071388526806e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 2181/2580 [17:43<03:07,  2.13it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    # \"distilbert-base-uncased\",num_labels=2, id2label=id2label, label2id=label2id)\n",
    "    \n",
    "    # Merge Labels with arguments and key_points for training dataset\n",
    "    arg_df_dev, kp_df_dev, labels_df_dev = load_kpm_data( subset=\"train\")\n",
    "    labels_df_dev=merge_df(arg_df_dev, kp_df_dev, labels_df_dev)\n",
    "    # Merge Labels with arguments and key_points for test dataset\n",
    "    arg_df_test, kp_df_test, labels_df_test = load_kpm_data(subset=\"dev\")\n",
    "    labels_df_test=merge_df(arg_df_test, kp_df_test, labels_df_test)\n",
    "    # labels_df_test=labels_df_test.merge(kp_df_test,on=\"key_point_id\",how=\"left\")\n",
    "    # labels_df_test=labels_df_test.merge(arg_df_test,on='arg_id',how=\"left\")\n",
    "    # labels_df_test.drop([\"stance_x\", \"topic_x\"], axis=1, inplace=True)\n",
    "    # labels_df_test=labels_df_test.rename(columns={\"stance_y\":\"stance\", \"topic_y\":\"topic\"})\n",
    "    #encode each sentence and append to dictionary\n",
    "    # filter where labels are not nan\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['argument'].notna()]\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['topic'].notna()]\n",
    "    # labels_df_dev = labels_df_dev[labels_df_dev['stance'].notna()]\n",
    "    labels_df_dev = remove_nan(labels_df_dev)\n",
    "    labels_df_test = remove_nan(labels_df_test)\n",
    "    # labels_df_test = labels_df_test[labels_df_test['argument'].notna()]\n",
    "    # labels_df_test = labels_df_test[labels_df_test['topic'].notna()]\n",
    "    # labels_df_test = labels_df_test[labels_df_test['stance'].notna()]\n",
    "    result_train = labels_df_dev.apply(preprocess_function,axis=1)    \n",
    "    result_test = labels_df_test.apply(preprocess_function,axis=1)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#     trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=result_train,\n",
    "#     eval_dataset=result_test,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "    trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=result_train,\n",
    "    eval_dataset=result_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    model_init=model_init,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "    train_result = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=20,\n",
    "    compute_objective=compute_objective,\n",
    ")\n",
    "    # train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (len(result_train))\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(result_train))\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "key_points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
